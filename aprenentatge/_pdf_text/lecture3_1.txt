Lecture 3.1
Supervised learning:
Bayesian and linear classifiers
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes


Contents
â€¢ Introduction
â€¢ Bayesian classification
â€¢ Estimation of probability density functions
â€¢ Linear discriminant functions and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

Introduction
â€¢ Supervised classification
â€“ It is about classifying a new sample in the correct class, having initially designed a 
classifier from the data available in a training set, in which, in particular, the 
samples are labeled with the class to which they belong.
Alberto Ortiz (last update 27/10/2025)

Contents
â€¢ Introduction
â€¢ Bayesian classification
â€¢ Estimation of probability density functions
â€¢ Linear discriminant functions and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

Bayesian classification
â€¢ The goal is to classify a new sample in the most likely class
â€“ Given a classification task in ğ‘€ classes, ğœ”1, ğœ”2, â€¦ , ğœ”ğ‘€, and a new sample ğ‘¥, 
we deal with:
â€“ The classifier decides the most likely class based on the maximum of the 
probabilities a posteriori: 
â€¢ Bayesian classification rule
Alberto Ortiz (last update 27/10/2025)

â€¢ Review of probability theory:
â€“ Probability function:
â€¢  ğ‘ assigns a value to each possible event ğ‘£ on the basis of how often that event 
occurs
â€¢ ï— can be stated as the set of events 
corresponding to a certain discrete 
random variable ğ‘‹ taking certain 
values, so that 
ğ‘(ğ´ğ‘–) = ğ‘(ğ‘‹ = ğ‘¥ğ‘–)
â€¢ Of particular relevance:
Bayesian classification
values taken by the random variable
probabilities of the 
corresponding values
ğ‘¥1 ğ‘¥2 ğ‘¥3 â€¦.    ğ‘¥ğ‘›
ğ‘2
ğ‘3
ğ‘1
ğ‘ğ‘›
ğ‘‹ ğ’‘ğ’Š = ğ’‘(ğ‘¿ = ğ’™ğ’Š)
ğ‘¥1 ğ‘1
ğ‘¥2 ğ‘2
â‹® â‹®
ğ‘¥ğ‘› ğ‘ğ‘›
Alberto Ortiz (last update 27/10/2025)

â€¢ Review of probability theory:
â€“ Law of total probability
â€¢
â€“ Bayes Rule
ïƒµ All this is verified under exactly the same conditions by substituting probabilities 
by probability density functions (pdf â€™s)
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification: two-class case (ğœ”1, ğœ”2)
â€“ Given the probabilities a priori of both classes ğ‘(ğœ”1) and ğ‘(ğœ”2)
â€¢ If you do not know ğ‘(ğœ”1) and ğ‘(ğœ”2), you can estimate them if necessary:
and the pdf (probability density functions) of each class
â€¢ If they are unknown, they have to be estimated 
from the available training data 
(we will deal with this topic later)
using Bayes' rule, it follows that:
total probability
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification: two-class case (ğœ”1, ğœ”2)
â€“ If the probabilities a priori are equal (ğ‘(ğœ”ğ‘–) = 1/ğ‘€ = 0.5), then the classification rule 
becomes dependent only on the pdfs of the classes:
â€“ In the one-dimensional case (1 feature), 
ğ‘¥0 is a threshold that partitions the space 
into two regions, ğ‘…1 and ğ‘…2
â€“ It is obvious that classification 
errors are unavoidable:
â€¢ Sample ğ‘¥ can be inside region ğ‘…2 and belong to the class Ï‰1 (same for ğ‘…1 and Ï‰2)
ğ‘(ğ‘¥|ğœ”1) ğ‘(ğ‘¥|ğœ”2)
R1 R2x0
x
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification: two-class case (ğœ”1, ğœ”2)
â€“ In the one-dimensional case, the probability of making a classification error is given by: 
THEOREM
The Bayesian classifier minimizes the likelihood of classification error
That is to say, if we move ğ‘¥0 left or right we will increase ğ‘ƒğ‘’
p(x|ï·1) p(x|ï·2)
R1 R2x0
x
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification: two-class case (ğœ”1, ğœ”2)
Proof (optimality of the Bayesian classifier)
On the one hand: 
On the other hand:
Therefore:
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

Bayesian classification
ğ‘¥1
ğ‘¥2
â€¢ Bayesian classification: 
two-class case (ğœ”1, ğœ”2) and 2 features ğ‘¥1 and ğ‘¥2
â€“ Minimize the probability
of error is equivalent to
partition the space of
features into M regions 
(as many as classes)
â€“ In general terms, 
regions ğ‘…ğ‘– and ğ‘…ğ‘— are 
separated by a decision curve 
that is described by the equation:
â€¢ Corresponds to the points of the
space in which the probabilities 
a posteriori coincide
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian Classification
â€“ Example Let us consider a problem of 2 equiprobable classes (ğ‘(ğœ”1) = ğ‘(ğœ”2) = 0.5) 
such that the class pdfs are Gaussians of variance 0.5 and means 0 and 1 respectively:
Calculate the optimal threshold ğ‘¥0 for minimum error probability.
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification for normal distributions
â€“ In the one-dimensional case:
â€“ We assume that the pdf of the classes obey the Gaussian L-dimensional distribution:
â€¢ For class ï·:
â€“ This distribution models properly many cases and is treatable mathematically and 
computationally, hence its popularity
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classification for normal distributions
â€“ Example
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ The goal is to derive the Bayesian classifier for the case
â€¢ Due to the exponential form of the pdf, it is preferable to work with the following 
discrimination functions ğ‘”ğ‘–(ğ‘¥), which involve the monotonous function ln(âˆ™):
â€¢ Finally:
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Case of 2 uncorrelated characteristics
â€¢ The decision rule is now given by the equation ğ‘”ğ‘– ğ‘¥  â€“ ğ‘”ğ‘— ğ‘¥ = 0
â€“ L = 2: ellipse, parabola, hyperbole, etc. â€“ conic, rule = 2D curve
â€“ L = 3: ellipsoid, paraboloid, hyperboloid, etc. â€“ quadric, rule = 3D surface
â€“ L > 3: hiperquadric
QUADRATIC
CLASSIFIER
Bayesian classification
ğ‘”ğ‘– ğ‘¥ > ğ‘”ğ‘— ğ‘¥  ïƒ ğ‘ ğœ”ğ‘– ğ‘¥ > ğ‘ ğœ”ğ‘— ğ‘¥  ïƒ ğ‘¥ â†’ ğœ”ğ‘–
Alberto Ortiz (last update 27/10/2025)
ğ‘”ğ‘– ğ‘¥ âˆ’ ğ‘”ğ‘— ğ‘¥ = ğ›¼ğ‘– âˆ’ ğ›¼ğ‘— ğ‘¥1
2 + ğ›½ğ‘– âˆ’ ğ›½ğ‘— ğ‘¥2
2 + ğ›¾ğ‘– âˆ’ ğ›¾ğ‘— ğ‘¥1 + ğ›¿ğ‘– âˆ’ ğ›¿ğ‘— ğ‘¥2 + ğœ–ğ‘– âˆ’ ğœ–ğ‘— > 0 ïƒ ğ‘¥ â†’ ğœ”ğ‘–


â€¢ Bayesian classifier for normal distributions
â€“ Example (equiprobable classes)
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Example (equiprobable classes)
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: decision hyperplanes
â€¢ If the classes have the same covariance matrix (Î£ = Î£ğ‘–) then the quadratic term and 
part of the constant term coincide in all discrimination functions:
â€¢ Therefore, they disappear from equations ğ‘”ğ‘– ğ‘¥  â€“ ğ‘”ğ‘— ğ‘¥ = 0. 
This allows us to define more useful and simpler discrimination functions:
â€¢ In this way, the discrimination functions are linear (and not quadratic) and the decision 
rule turns out to be a decision hyperplane: (2D) a straight line, (3D) a plane, ...
â€¢ Let us have a look at two cases of the covariance matrix: (1) ï“ = ğœ2ğ¼ and (2) any ï“
LINEAR
CLASSIFIER
Bayesian classification
ğ‘”ğ‘– ğ‘¥ > ğ‘”ğ‘— ğ‘¥ , âˆ€ğ‘— 
ïƒ  ğ‘ ğœ”ğ‘– ğ‘¥ > ğ‘ ğœ”ğ‘— ğ‘¥ , âˆ€ğ‘— ïƒ ğ‘¥ â†’ ğœ”ğ‘–
Alberto Ortiz (last update 27/10/2025)
ğ‘”1 ğ‘¥ âˆ’ ğ‘”2 ğ‘¥ = ğ›¼1 âˆ’ ğ›¼2 ğ‘¥1 + ğ›½1 âˆ’ ğ›½2 ğ‘¥2 + ğ›¾1 âˆ’ ğ›¾2 > 0 ïƒ ğ‘¥ â†’ ğœ”1
= ğ›¼ğ‘–ğ‘¥1 + ğ›½ğ‘–ğ‘¥2 + ğ›¾ğ‘– (case of 2 features)
2D case
2 classes

â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: Î£ = ğœ2ğ¼
â€¢ Then, the discrimination functions take the following form:
so that the decision rules can be written as:
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: Î£ = ğœ2ğ¼
â€¢ Decision rule
â€¢ Two-feature case
â€¢ Which is this straight line?
Bayesian classification
Alberto Ortiz (last update 27/10/2025)


â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: Î£ = ğœ2ğ¼
â€¢ Decision rule for the two-feature case
â€¢ Any point x such that x â€“ x0
is orthogonal to ï­i - ï­j
belongs to the straight line
â€¢ x0 is always along
the vector ï­i - ï­j
â€“ if p(ï·i) = p(ï·j), x0 is the middle
point between ï­i and ï­j
â€“ if p(ï·i) < p(ï·j), x0
moves towards ï­i
along vector ï­i - ï­j
x2
x1
ï­i
ï­j
ï­i - ï­j
x0
x
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: Î£ = ğœ2ğ¼
â€¢ the circles expand to 3ï³ ï‚º 98%
Bayesian classification
class boundary class boundary
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: any Î£
â€¢ We recover the original linear discrimination functions:
â€¢ Then:
Bayesian classification
Alberto Ortiz (last update 27/10/2025)


â€¢ Bayesian classifier for normal distributions
â€“ Classes with the same covariance matrix: any Î£
â€¢ The decision hyperplane is no longer necessarily 
orthogonal to ï­i - ï­j but to ï“-1(ï­i - ï­j)
â€“ ï“-1(ï­i - ï­j) is the result of transforming (ï­i - ï­j) through the 
matrix ï“-1
â€¢ x0 is always on the vector ï­i - ï­j
â€“ if p(ï·i) = p(ï·j), x0 is the average of ï­i y ï­j
â€“ if p(ï·i) < p(ï·j), x0 moves towards ï­i along ï­i - ï­j
x2
x1
ï­i
ï­j
ï­i - ï­j
x0
x
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Example In a two-dimensional classification problem with two equiprobable classes, the 
classes follow two normal distributions with the following parameters:
Classify the sample ğ‘¥ = (1.0,2.2)ğ‘‡ using a Bayesian classifier.
Therefore, ğ‘¥ â†’ ğœ”1. 
Bayesian classification
Alberto Ortiz (last update 27/10/2025)


> 0 > 0 > 0
â€¢ Bayesian classifier for normal distributions
â€“ So far we have considered two-class cases and derived the boundaries between 
class pairs through the discrimination functions: 
â€“ For two classes
â€“ If there are more than 2 classes, we have to determine ğ‘”ğ‘– | ğ‘”ğ‘– > ğ‘”ğ‘— , ï€¢ğ‘— â‰  ğ‘–. 
In case of using the joint discrimination functions ğ‘”ğ‘–ğ‘— to decide, we have to 
consider several ğ‘”ğ‘–ğ‘—. For example, for 3 classes:
â€¢ to determine Ri we need several ğ‘”ğ‘–ğ‘˜ , ğ‘”ğ‘˜ğ‘–
M classes: M-1 pairs
 R1 (ï·1): g12 > 0
              g13 > 0
R3 (ï·3):
g13 < 0
g23 < 0
R2 (ï·2):
g12 < 0
g23 > 0
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Minimum distance classifiers
â€¢ We can see the above from another point of view
â€¢ We assume equiprobable classes with the same covariance matrix. Then:
â€¢ We assign x to the class for which the probability is greater ïƒ gi(x) > gj(x) ï€¢j ï‚¹ i
â€“ (1) If ï“ = ï³2I, gi(x) is higher the closer it is x to ï­i
ïƒ assign x to the class whose center ï­i is closer (Euclidean distance)
â€“ (2) For generic ï“, we have to assign x to the class for which the following expression takes a 
lowest value:
â–ª dm ï‚º Mahalanobis distance (considers the scattering present in the features)
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

â€¢ Bayesian classifier for normal distributions
â€“ Example In a two-dimensional classification problem into two equiprobable 
classes the classes have two normal distributions with the following parameters:
Classify the vector x = (1.0,2.2)T using a Bayesian classifier.
â€¢ dm(x,ï­1) < dm(x,ï­2) ïƒ x â†’ ï·1 . 
â€¢ REMARK: the Euclidean distances would be de(x,ï­1) = 2.417 and de(x,ï­2) = 2.154, so, 
if we used them, we would assign x â†’ ï·2 .
Bayesian classification
Alberto Ortiz (last update 27/10/2025)

Contents
â€¢ Introduction
â€¢ Bayesian classification
â€¢ Estimation of probability density functions
â€¢ Linear discriminant functions and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions (pdfs)
â€“ The Bayesian classifier assumes that we have knowledge on the pdfs of the 
classes of the problem
â€“ There are different methods to get this type of information:
â€¢ The expression of the pdf is known but the parameters are unknown
â†’ parametric estimation
â€“ Maximum likelihood estimators
â€“ others
â€¢ The expression of the pdf is not known â†’ non-parametric estimation
â€“ Parzen windows method
â€“ k-nearest neighbours method (KNN)
â€“ others
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Maximum likelihood estimators
â€¢ Let us consider an M-class classification problem whose samples are distributed in 
accordance to ğ’‘ ğ’™ ğğ’Š; ğœ½ğ’Š , ğ’Š = ğŸ, â€¦ , ğ‘´, where ğœ½ğ’Š is the vector of parameters for 
class ğğ’Š
â€“ It's about estimating ğœ½ğ’Š by means of a set of samples ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘˜ from class ğğ’Š
â€¢ We assume that the samples of one class do not affect the estimation of parameters 
for the other classes in order to formulate the problem irrespective of the class
ïƒ the estimation is repeated for each class
â€¢ In this way, given the statistically independent samples ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘ from ğ’‘ ğ’™ğ’Œ ğœ½ , 
we calculate the following joint pdf:
â€¢ Then, the maximum likelihood estimator of ğœƒ is given by:
â€“ This represents that ğœƒ that better explains samples ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Maximum likelihood estimators
â€¢ To simplify the calculations we will go once again to the function ln(ïƒ—) to define the log-
likelihood function:
â€¢ Now, you can find the derivative of the log-likelihood and equal to 0:
â€¢ For sufficiently high N values, the maximum likelihood estimator is asymptotically 
unbiased, follows a normal distribution and exhibits minimal variance
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Maximum likelihood estimators
â€¢ L-dimensional Gaussian distribution with ï“ known
â€¢ L-dimensional Gaussian distribution, ï­ and ï“ unknown
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

for different 
sizes of ğ‘…ğ‘—
â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: first approximation
â€¢ It is about estimating a certain pdf p(x) without setting any expression for the pdf
â€¢ Let us assume we have N independent samples ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘ that come from the pdf 
that we want to estimate 
â€¢ To this end, we build a histogram using bins ğ‘…ğ‘— of the same size:
â€“ ğ‘˜ğ‘,ğ‘…ğ‘— = how many of the N samples belong to bin ğ‘…ğ‘—
â€“ {ğ‘˜ğ‘,ğ‘…ğ‘—/ğ‘} is an aproximation of ğ‘ƒğ‘…ğ‘— = probability that ğ‘¥ belongs to ğ‘…ğ‘—:
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: first approximation
â€¢ We now define sufficiently small regions ğ‘… around ğ‘¥, aiming at calculating ğ‘(ğ‘¥). 
In this regard, if ğ‘(ğ‘¥) is assumed constant inside ğ‘…:
â€“ ğ‘‰ is the (hyper)volume occupied by region ğ‘… (1D â€“ length, 2D â€“ area, 3D â€“ volume, etc.)
â€“ e.g. if ğ‘… is a (hyper)cube of dimension ğ¿ and side length â„, ğ‘‰ = â„ğ¿
â€¢ Therefore:
â€¢  ğ‘ğ‘,ğ‘…(ğ‘¥) â†’ ğ‘(ğ‘¥) as ğ‘ â†’ ï‚¥ if the following holds:
â€“ ğ‘‰ â†’ 0 (small regions)
â€“ ğ‘˜ğ‘,ğ‘… â†’ âˆ (sufficient number of samples in each ğ‘…)
â€“ ğ‘˜ğ‘,ğ‘…/ğ‘ â†’ 0 (high total number of samples)
â€¢ In short: for each ğ‘¥, ğ‘(ğ‘¥) is approximated by defining 
a small ğ‘… region around ğ‘¥ and counting how many ğ‘¥ğ‘–
fall into ğ‘… (= ğ‘˜ğ‘,ğ‘…); if ğ‘˜ğ‘,ğ‘… â†‘â†‘ and ğ‘ â†‘â†‘â†‘, then ğ‘ğ‘,ğ‘…(ğ‘¥) â‰ˆ ğ‘(ğ‘¥)
ğ‘¥
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ EstimaciÃ³n de funciones de densidad de probabilidad
â€“ Non-parametric estimation: Parzen windows (Parzen, 1962)
â€¢ Let us consider a region ğ‘… shaped like a (hyper) ğ¿-dimensional cube of side â„ğ‘. 
Then:
â€¢ Let us consider the following function (box function or kernel):
â€¢ So, for a certain ğ’™:
â€¢ Then, the number of samples that are inside the ğ‘¥-centered (hyper)cube is:
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ At last:
â€¢ Therefore, given a certain ğ‘¥, to obtain the estimate of ğ‘(ğ‘¥): (1D case)
def parzen_box_1D(x,X,h):
# x = point where to evaluate the PDF
# X = table of samples
# h = side of the (hyper)cube
N = X.shape[0]; kn = 0
for i in range(N):
if abs((x - X[i])/h) <= 0.5: 
kn = kn+1
p = (kn/N)/h
return p
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ Comments on ğ‘ğ‘:
â€“ ğ‘ğ‘ is a legitimate pdf
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ Comments on ğ‘ğ‘:
â€“ function ğ‘ğ‘(ğ‘¥) can be seen as the average of N functions centred in the samples ğ‘¥ğ‘–
â€“ if â„ğ‘ is large, the amplitude of ğ›¿ğ‘
is small and ğ‘ğ‘ is the superposition 
of ğ‘ wide pulses
â€“ if â„ğ‘ is small, the amplitude of ğ›¿ğ‘
is large and ğ‘ğ‘ is the superposition 
of ğ‘ narrow pulses
â€“ â„ğ‘ â†’ 0 ïƒ ğ›¿ğ‘ â†’ ğ›¿
ï¤N
1 ï¤N
2 ï¤N
3 ï¤N
4
x1 x2 x3 x4 x5
ï¤N
5
x1 x2 x3 x4 x5
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ Example: ğ‘ random values extracted from a distribution ğ‘(0,1) â€“ rectangular kernel
softer
noisier
better estimation â†’
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ As we have already seen in the previous example, when approximating continuous 
functions [ğ‘(ïƒ—)] by discontinuous kernel functions [ïª(ïƒ—)], the resulting estimate also 
presents discontinuities
â€¢ To avoid this, it is suggested to use continuous kernels ïª(ïƒ—)
â€“ It can be shown that the resulting estimate ğ‘ğ‘(ğ‘¥) is a legitimate pdf if:
â€“ One of the most commonly used kernels 
is the Gaussian kernel (mean 0, variance 1): 
Estimation of probability density functions
def parzen_gauss_1D(x,X,h):
# x = point where to evaluate the PDF
# X = data samples
# h = side of the (hyper)cube
N = X.shape[0]; kn = 0
for i in range(N):
kn += 1/(sqrt(2*pi))*exp(-0.5*((x-X[i])/h)**2)
p = (kn/N)/h
return p
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ Now ğ‘ğ‘(ğ‘¥) is obtained as the average of ğ‘ Gaussians centered on the samples ğ‘¥ğ‘–
â€¢ Effect of varying bandwidth â„ğ‘
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: Parzen windows
â€¢ Example: ğ‘ random values extracted from a distribution ğ‘(0,1) â€“ Gaussian kernel
better estimation â†’
Estimation of probability density functions
softer
noisier
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: k nearest neighbours
â€¢ Given ğ‘¥ and a collection of samples ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘ from a certain pdf p(x), to estimate 
p(x):
â€“ Parzen windows method â€“ first, a search volume is set around ğ‘¥, ğ‘‰ğ‘, and next we 
determine the number of samples ğ‘˜ğ‘ belonging to that volume
â€“ Method of the k nearest neighbors â€“ we first find the ğ‘˜ğ‘ samples nearest to ğ‘¥, and next 
we determine the minimal volume ğ‘‰ğ‘˜ğ‘ where they are contained in
We set VN and find kN â†’
We set kN and find VN â†’
Estimation of probability density functions
def knn_1D(x,X,k):
# x = point where to evaluate the PDF
# X = data samples
# k = number of neighbours
N = X.shape[0]; d = []
for i in range(N):
d.append(abs(x-X[i]))
d.sort()
V = 2 * d[min(N,k)-1]
p = (k/N)/V
return p
Alberto Ortiz (last update 27/10/2025)

â€¢ Estimation of probability density functions
â€“ Non-parametric estimation: k nearest neighbours
â€¢ Example: N random values sampled from a distribution ğ‘(0,1)
â€¢ In this case, it has been used:  
â€¢ In the case of Parzen windows, it is suggested to use: 
best estimate â†’
Estimation of probability density functions
Alberto Ortiz (last update 27/10/2025)

Contents
â€¢ Introduction
â€¢ Bayesian classification
â€¢ Estimation of probability density functions
â€¢ Linear discriminant functions and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ We have already seen that, depending on the pdf â€™s of the classes (Gaussian case), a 
Bayesian classifier can derive in a set of linear discrimination functions. For 
example, for 2 classes:
â€“ simple and computationally very interesting classifier
â€¢ In this section, we concentrate again on linear discrimination functions, but from a 
different perspective: we do not assume any pdf for the classes
â€“ Therefore, regardless of the pdf of the classes, we expect them to be separable by 
(hyper)planes (1D â€“ point, 2D â€“ straight line, 3D â€“ plane, etc.)
â€¢ In this case it is said that the classes are linearly separable
â€“ We will see how you can find a (hyper)plane that separates the classes from each other 
(perceptron algorithm)
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ Linear discrimination functions
â€“ Goal: find a (hyper)plane that allows us to separate the training samples in 2 classes
â€“ For example, for ğ¿ = 2 features:
(hyper)plane: ğ‘¥ | ğ‘”12(ğ‘¥) = ğ‘¤ğ‘‡(ğ‘¥ âˆ’ ğ‘¥0) = 0
Linear discrimination functions 
and the perceptron algorithm
x2
x1
-w0/w2
-w0/w1
w
x0
x
(normal form)
(slope-intercept 
 form)
Alberto Ortiz (last update 27/10/2025)

â€¢ Linear discrimination functions. For now, we will only consider hyperplanes which go 
through the origin
â€“ This means that ğ‘¥0 = (0,0)ğ‘‡ and thus in ğ‘¤ğ‘‡(ğ‘¥ âˆ’ ğ‘¥0)
â€“ For any point ğ‘¥ğ‘– outside the hyperplane we can state:
â€“ The sign of ğ‘‘ depends on the relative position of ğ‘¥ğ‘– with 
regard to the hyperplane:
â€“ |ğ‘‘| = |ğ‘¤ğ‘‡ğ‘¥ğ‘–| indicates how far away the sample is from 
the (hyper)plane of discrimination
x2
x1
x
w
0Âº
x2
x1
xi
d
w
ï¢
d > 0
d < 0
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ We assume that the classes ğœ”1 and ğœ”2 are linearly separable, i.e. the hyperplane exists
â€¢ The goal is thus to find a function ğ‘”12 ğ‘¥ = ğ‘¤ğ‘‡ğ‘¥ such that ğ‘”12 ğ‘¥ is as follows for each 
sample ğ‘¥ğ‘– of the training set:
â€“ ğ‘”12 defined in this way is also named as a 
discrimination function, which in this case turns out to 
be linear, and thus it is a linear discrimination function
â€¢ To find the hyperplane, we consider the following function (perceptron cost):
where:
x2
x1
xi
d
w
ï¢
d > 0
d < 0
Linear discrimination functions 
and the perceptron algorithm
â€¢ Change of classroom for 
exam: classroom 5A
â€¢ Bring your own 
calculator
Alberto Ortiz (last update 27/10/2025)

â€¢ The perceptron algorithm (Rosenblatt, 1950s) is able to find, through the next 
iterative approach, the required hyperplane: (sort of gradient descent)
â€¢ The algorithm converges if the classes are linearly separable and if the sequence of 
learning factor values ğœŒğ‘¡ meets certain conditions:
where ğ‘¡ denotes iteration number.
âˆ’ The sequence ğœŒğ‘¡ determines the convergence speed.
âˆ’ For instance, ğœŒğ‘¡ = ğ‘/ğ‘¡ and ğœŒğ‘¡ = ğœŒ (ğœŒ bounded) meet those conditions.
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ Details on the use of the (basic) perceptron algorithm:
(1a) To deal with hyperplanes that do not contain the origin, the feature vectors must be 
augmented in one additional dimension ğ‘¥ğ‘–
âˆ— = (ğ‘¥ğ‘–, 1)ğ‘‡. In this way:
â– We will use ğ‘¤ğ‘‡ instead of (ğ‘¤âˆ—)ğ‘‡ and ğ‘¥ instead of ğ‘¥âˆ— to simplify the notation
(1b) The rule for modifying ğ‘¤ within every iteration can be generically implemented as 
follows:
Linear discrimination functions 
and the perceptron algorithm
1 iteration
Alberto Ortiz (last update 27/10/2025)

â€¢ Details of the (basic) perceptron algorithm:
(1) Example 1
â€“ The dashed line corresponds to:
where ğ‘¤(ğ‘¡) =  (1,1, âˆ’0.5)ğ‘‡ is the result of
the previous step of the perceptron 
algorithm using ğœŒğ‘¡ = ğœŒ = 0.7
â€“ The incorrectly classified samples are:
(0.40,0.05)ğ‘‡ and (âˆ’0.20,0.75)ğ‘‡
â€“ The new iteration yields
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)


â€¢ Details of the (basic) perceptron algorithm:
(1) Example 1
â€“ The dashed line corresponds to:
where ğ‘¤(ğ‘¡) =  (1,1, âˆ’0.5)ğ‘‡ is the result of
the previous step of the perceptron 
algorithm using ğœŒğ‘¡ = ğœŒ = 0.7
â€“ The incorrectly classified samples are:
(0.40,0.05)ğ‘‡ and (âˆ’0.20,0.75)ğ‘‡
â€“ The new iteration yields
â€“ The resulting hyperplane classifies correctly all the samples and the algorithm 
ends with:
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)


â€¢ Details of the (basic) perceptron algorithm:
(1) Example 2
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ Details of the (basic) perceptron algorithm:
(1) Implementation
Linear discrimination functions 
and the perceptron algorithm
def perceptron_2D(X, y, rho, nit):
N = X.shape[0]
w = np.zeros(3)
for t in range(nit):
S = np.zeros(3); ic = 0
for i in range(N):
xs = [X[i,0], X[i,1], 1]
if np.dot(w,xs) < 0 and y[i] == 1: # 1 ï‚º w1
S = S + xs; ic += 1
elif np.dot(w,xs) >= 0 and y[i] == 0: # 0 ï‚º w2
S = S - xs; ic += 1
if ic == 0: # Y = empty set
break
else:
w = w + rho * S
w = w / sqrt(w[0] ** 2 + w[1] ** 2)
return w
Alberto Ortiz (last update 27/10/2025)

â€¢ Details of the (basic) perceptron algorithm:
(2) The pocket algorithm: dealing with non-linearly separable datasets
â€“ Stops after a number of iterations (ğ‘‡), providing the best hyperplane which has 
been found along that number of iterations
â€“ Partially solves the convergence problem of the original perceptron algorithm 
when the classes are not linearly separable
Linear discrimination functions 
and the perceptron algorithm
Alberto Ortiz (last update 27/10/2025)

â€¢ Classification device:
â€“ Once the perceptron algorithm has converged with e.g. ğ‘¤ = (ğ‘¤1, ğ‘¤2, â€¦ , ğ‘¤ğ¿, ğ‘¤0), then 
the following structure can be considered to implement the classification operation:
â€“ The perceptron can be considered as the basic building element for more complex learning 
machines, e.g. neural networks
ï“
input 
nodes
synaptic
weights
threshold
wL
w2
w0
w1
x1
x2
xL
activation
function
f
perceptron or
[artificial] neuron
(McCulloch-Pitts, 1943)
Linear discrimination functions 
and the perceptron algorithm
ğ‘¥ = (ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ¿)
Alberto Ortiz (last update 27/10/2025)

Lecture 3.1
Supervised learning:
Bayesian and linear classifiers
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes
