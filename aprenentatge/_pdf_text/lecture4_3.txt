Instance-based learning:
k-Nearest Neighbours
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes

Alberto Ortiz (last update 15/12/2025) 2
Contents
â€¢ k-Nearest Neighbours classifier
â€¢ Supplementary material: Nearest-Neighbour Search & k-d trees
â€¢ Condensed Nearest Neighbours
â€¢ Example of use

Alberto Ortiz (last update 15/12/2025) 3
k-Nearest Neighbours classifier
â€¢ Supervised classification scheme based on the so-called k-nearest neighours rule:
â€“ Given an unknown sample ğ‘¥ and a distance ğ‘‘(ğ‘¥, ğ‘¦), e.g. Euclidean ğ‘‘(ğ‘¥, ğ‘¦) = ğ‘¥ âˆ’ ğ‘¦
1. identify the ğ‘˜ nearest neighbours (according to ğ‘‘) out of the ğ‘ training samples
2. out of these ğ‘˜ samples, identify the number of patterns ğ‘›ğ‘–
that belong to every class ğœ”ğ‘–, ğ‘– = 1, 2, â€¦ , ğ‘€
3. assign ğ‘¥ to the class ğœ”ğ‘— such that ğ‘›ğ‘— = max{ğ‘›1, ğ‘›2, â€¦ , ğ‘›ğ‘˜}
â€¢ kNN is considered a lazy learning algorithm
â€“ There is no training, or data abstraction/modeling, step
â€“ Defers data processing until it receives a request to classify an unlabelled sample
â€“ The full training dataset is needed, but the classification depends only on the ğ‘˜ neighbours
â€¢ There is a single parameter ğ‘˜
?
ï·1
ï·2
ï·3
ï·4
x â†’ ï·1
k = 5

4
k-Nearest Neighbours classifier
â€¢ Examples of distance functions:
â€“ weighted ğ‘³ğ’‘ metric (or Minkowski measure)
Alberto Ortiz (last update 15/12/2025)


Alberto Ortiz (last update 15/12/2025) 5
k-Nearest Neighbours classifier
â€¢ To avoid ties, k is typically chosen odd for two-class problems, and, in general, not to 
be a multiple of the number of classes M
â€¢ Even with this, ties may arise: ğ‘›1(ğ‘¥) = ğ‘›4(ğ‘¥)
â€“ Ties may be broken arbitrarily
â€“ The unlabeled sample x may be assigned to the 
class of the nearest neighbor
â€¢ consider all classes
â€¢ consider only the classes with the tying values
â€“ Instead of every sample voting 1, make use of a 
weighted vote inversely proportional 
to the distance:
?
ï·1
ï·2
ï·3
ï·4
x â†’ ï·3
x â†’ ï·4
x â†’ ï·4
?
ï·1
ï·2
ï·3
ï·4
k = 5 k = 5

Alberto Ortiz (last update 15/12/2025) 6
k-Nearest Neighbours classifier
â€¢ An example: 3 classes, 60 samples/class, white = unclassified, i.e. kNN voting tied
5NN
classification map
original dataset

Alberto Ortiz (last update 15/12/2025) 7
k-Nearest Neighbours classifier
â€¢ Nearest neighbour is competitive:
â€¢ Other theoretical results: 
â€“ e.g. error is less than twice the optimal classification error (Bayes error)
Yann LeCunn â€“ MNIST Digit Recognition
â€“ Handwritten digits
â€“ 28x28 pixel images: d = 784
â€“ 60,000 training samples
â€“ 10,000 test samples
(http://yann.lecun.com/exdb/mnist/)
Test Error Rate (%)
Linear classifier (1-layer NN) 12.0
3-nearest-neighbors, Euclidean 5.0
3-nearest-neighbors, Euclidean, deskewed 2.4
1-NN, Tangent Distance, 16x16 1.1
1000 RBF + linear classifier (10 neurons) 3.6
SVM deg 4 polynomial 1.1
2-layer NN, 300 hidden units 4.7
2-layer NN, 300 HU, [deskewing] 1.6
LeNet-5, [distortions] 0.8
Boosted LeNet-4, [distortions] 0.7


Alberto Ortiz (last update 15/12/2025) 8
k-Nearest Neighbours classifier
â€¢ The simplest version of the algorithm is the nearest neighbour classifier (1NN)
Assign ğ‘¥ to the class ğ‘¤ğ‘— of its nearest neighbour
â€¢ Systematic application of the rule throughout the feature space gives rise to the 
Voronoi tessellation/diagram
â€“ shows the points of the feature space which are closest to every sample and therefore 
â€œinheritâ€ its label
1NN
Voronoi tesselation
ties are less likelier !!

Alberto Ortiz (last update 15/12/2025) 9
k-Nearest Neighbours classifier
â€¢ In the kNN algorithm, the greatest effort is placed on the classification not on the 
training:
â€“ Given a pattern ğ‘¥ to classify, one has to calculate the distance ğ‘‘(ğ‘¥, ğ‘¥ğ‘–) from ğ‘¥ 
to any of the ğ‘ patterns ğ‘¥ğ‘– in the full dataset and keep the ğ‘˜ nearest patterns 
according to ğ‘‘ (brute force approach)
â€“ High computational cost for ğ‘ large: e.g. ğ‘˜ = 1, ğ‘ = 106 ïƒ 106 comparisons
â€¢ kNN and 1NN are similar in terms of efficiency
â€“ retrieving the ğ‘˜ nearest neighbors is not much more expensive than retrieving a 
single nearest neighbor
â€“ ğ‘˜ nearest neighbors can be maintained in a sorted queue
â€¢ A number of ways of reducing the search cost
â€“ Handle the dataset with the goal of reducing the search complexity, e.g. use a k-d tree or 
a ball-tree, use approximate nearest neighbour search (ANN), etc.
â€“ Reduce the size of the dataset without significantly altering the classification accuracy

Alberto Ortiz (last update 15/12/2025) 10
Contents
â€¢ k-Nearest Neighbours classifier
â€¢ Supplementary material: Nearest-Neighbour Search & k-d trees
â€¢ Condensed Nearest Neighbours
â€¢ Example of use

Alberto Ortiz (last update 15/12/2025) 11
k-d trees
â€¢ A k-d tree is a binary tree to store a set of k-dimensional points in an â€œorderedâ€ way 
â€¢ Every non-leaf node generates a splitting hyperplane along one axis, which divides 
the space into two parts, known as half-spaces
â€“ Points to the left of the hyperplane (for that axis) fall within the left subtree and 
points to the right of the hyperplane (for that axis) fall within the right subtree 
â€“ The splitting axis is chosen such that every node is associated with one of the k 
dimensions, with the hyperplane perpendicular to that dimension's axis 
â€¢ e.g. if for a particular split the x1 axis is chosen, all points in the subtree with a smaller x1 value 
than the nodeâ€™s x1 value will appear in the left subtree and all points with larger x1 value will be in 
the right subtree. In such a case, the hyperplane would be defined by the x1-value of the node, 
and its normal would be the unit x1-axis.
(7,2)
(5,4) (9,6)
(8,1)(2,3) (4,7)
x1
x2
X = {(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}
ï‚Œ [x1] 
(7,2)
ï‚ [x2] 
(5,4)
ï‚ [x2] 
(9,6)

Alberto Ortiz (last update 15/12/2025) 12
k-d trees
â€¢ There are many possible ways to choose axis-aligned splitting planes, and so there 
are many different ways to construct k-d trees
â€¢ The canonical method of k-d tree construction has the following constraints:
â€“ The splitting axis changes sequentially from level to level: 
first level â€“ x1, second level â€“ x2, etc. (start again with x1 when the k-th level is reached)
â€“ Split is performed at the median of the subtree values for the chosen axis
â€“ This method leads to a balanced k-d tree: all leaf nodes approx. equally closer to the root
(7,2)
(5,4) (9,6)
(8,1)(2,3) (4,7)
x1
x2
X = {(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}
(2) [L] choose axis x2 and take the median of
 {3,4,7} â†’ 4 â†’ (5,4)
 left subtree: (2,3)
 right subtree: (4,7)
(1) choose axis x1 and take the median of
 {2,5,9,4,8,7} â†’ {2,4,5,7,8,9} â†’ 7 â†’ (7,2)
 left subtree: {(2,3), (5,4), (4,7)}
 right subtree: {(9,6), (8,1)}
(3) [R] choose axis x2 and take the median of 
 {6,1} â†’ {1,6} â†’ 6 â†’ (9,6)
left subtree: (8,1)

Alberto Ortiz (last update 15/12/2025) 13
k-d trees
â€¢ Another popular method:
â€“ Choose the splitting axis according to the spread (var / range) of the data along each axis
â€“ Choose the split value as the average of values for the chosen axis and subtree
â€“ This method avoids the sort operation to find the median
â€“ Besides, samples are stored only at the leafs, not throughout the tree
X = {(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}
(2L) s1 = range{2,5,4} = 5 - 2 = 3, ï­1 = 3.67
 s2 = range{3,4,7} = 7 - 3 = 4, ï­2 = 4.67
 choose axis x2 and split at x2 = 4.67
 left subtree: {(2,3),(5,4)}
 right subtree: (4,7)
(1) s1 = range{2,5,9,4,8,7} = 9 - 2 = 7, ï­1 = 5.83
 s2 = range{3,4,6,7,1,2} = 7 - 1 = 6, ï­2 = 3.83
 choose axis x1 and split at x1 = 5.83
 left subtree: {(2,3), (5,4), (4,7)}
 right subtree: {(9,6), (8,1),(7,2)}
(3R) s1 = range{9,8,7} = 9 - 7 = 2, ï­1 = 8
 s2 = range{6,1,2} = 6 - 1 = 5, ï­2 = 3
 choose axis x2 and split at x2 = 3
 left subtree: {(8,1), (7,2)}
 right subtree: (9,6)
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)

Alberto Ortiz (last update 15/12/2025) 14
k-d trees
â€¢ Growth of the tree can be stopped at any level: there can be more than 1 sample per leaf
â€¢ Growth can also continue until the number of samples per leaf is below a threshold
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
ï‚Œ
ï‚Œ
ï‚
ï‚
ï‚
ï‚
ï‚ ï‚
ï‚ ï‚

Alberto Ortiz (last update 15/12/2025) 15
k-d trees
â€¢ Nearest-neighbor search (2nd k-d tree construction approach):
â€¢ remark: algorithm slightly different when there are samples at intermediate nodes
node {
int axis; // splitting axis
real value; // splitting value
node left; // left subtree
node right; // right subtree
kdpoint point; // if leaf node
}
NNS(q:in kdpoint, n:in node, b:inout point, r:inout real) 
{
if n.left = empty & n.right = empty then // it is a leaf node (only 1 sample/leaf)
r_ = dist(q, n.point);
if r_ < r then r = r_; b = n.point;  // update nearest 
else // find first subtree to look into
if q(n.axis) <= n.value then // use splitting axis, visit subtrees in proper order
NNS(q, n.left, b, r);  // look within left subtree
if q(n.axis) + r >  n.value then NNS(q, n.right, b, r); // NN can be in right subtree
else
NNS(q, n.right, b, r); // look within right subtree
if q(n.axis) â€“ r <= n.value then NNS(q, n.left, b, r);  // NN can be in left subtree
}
initial call: NNS(q, root, b, inf);
search
pruning

Alberto Ortiz (last update 15/12/2025) 16
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83

Alberto Ortiz (last update 15/12/2025) 17
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3

Alberto Ortiz (last update 15/12/2025) 18
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3
1 < 1.5

Alberto Ortiz (last update 15/12/2025) 19
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3
1 < 1.5
r1 = 2

Alberto Ortiz (last update 15/12/2025) 20
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3
1 < 1.5
r1 = 2
L: 1+2 > 1.5

Alberto Ortiz (last update 15/12/2025) 21
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3
1 < 1.5
r1 = 2
L: 1+2 > 1.5
r2 = 1.4

Alberto Ortiz (last update 15/12/2025) 22
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
1 < 3
1 < 1.5
r1 = 2
L: 1+2 > 1.5
r2 = 1.4
L: 1+1.4 < 3

Alberto Ortiz (last update 15/12/2025) 23
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
r1 = 2 r2 = 1.4

Alberto Ortiz (last update 15/12/2025) 24
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
1 < 4.67
r1 = 2 r2 = 1.4

Alberto Ortiz (last update 15/12/2025) 25
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
1 < 4.67
r1 = 2 r2 = 1.4
6 > 3.5

Alberto Ortiz (last update 15/12/2025) 26
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
1 < 4.67
r1 = 2 r2 = 1.4
6 > 3.5
d = 3.16 > 1.4

Alberto Ortiz (last update 15/12/2025) 27
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
1 < 4.67
r1 = 2 r2 = 1.4
6 > 3.5
d = 3.16 > 1.4
R: 6-1.4 > 3.5

Alberto Ortiz (last update 15/12/2025) 28
k-d trees
â€¢ q = (6,1)
â€¢ r0 = ï‚¥
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
6 > 5.83
R: 6 - 1.4 < 5.83
1 < 4.67
r1 = 2 r2 = 1.4
6 > 3.5
d = 3.16 > 1.4
R: 6-1.4 > 3.5
L: 1+1.4 < 4.67

Alberto Ortiz (last update 15/12/2025) 29
k-d trees
â€¢ q = (6,1) â†’ NN = (7,2), dist = 1.4
â€¢ r0 = ï‚¥
â€¢ 11 nodes in the tree, 8 visited, 3 distance calculations
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
r1 = 2 r2 = 1.4d = 3.16 > 1.4

Alberto Ortiz (last update 15/12/2025) 30
k-d trees
â€¢ Another example: q = (9,2) 
â€¢ NN = (8,1), dist = 1.4
â€¢ 11 nodes in the tree, 6 visited
2 distance calculations
x1 = 5.83
x2 = 4.67 x2 = 3
(4,7) (9,6)x1 = 3.5 x2 = 1.5
(7,2)(8,1)(5,4)(2,3)
r2 = 1.4 r1 = 2
2 â€“ 2 < 1.5
2 + 1.4 < 3
d = 4 > 1.4
9 â€“ 1.4 > 5.83

Alberto Ortiz (last update 15/12/2025) 31
k-d trees
â€¢ A larger example:
â€“ N = 106 samples randomly generated
â€“ searching for q = (0.29514, 0.897237, 0.941998) randomly generated
â€“ found NN = (0.296093, 0.896173, 0.948082) at distance 0.00624896
â€“ visited 44 nodes
â€¢ For 1NN, time complexity is O(N) in the worst case, but on average is O(log2 N)
â€¢ If there are multiple samples at the leaves, then the nearest neighbor must be 
searched among all of them:
nearest(q:in kdpoint, n:in node, b:inout point, r:inout real) 
{
if n.left = empty & n.right = empty then // it is a leaf node (m samples/leaf)
for every sample i at node n
r_ = dist(q, n.points[i]);
if r_ < r then r = r_; b = n.points[i];  // update nearest 
[â€¦]
}
node {
int axis; // splitting axis
real value; // splitting value
node left; // left subtree
node right; // right subtree
kdpoint points[m]; // if leaf node, a maximum of m nodes
}

Alberto Ortiz (last update 15/12/2025) 32
k-d trees
â€¢ The algorithm can be extended in several ways by simple modifications: 
â€“ To search for the k nearest neighbours
â€¢ maintain k current best instead of just one
â€¢ prune a branch search only when k points have been found and the branch cannot 
have points closer than any of the k current bests
â€¢ For kNN , time complexity is O(kN) in the worst case, but on average is O(k log2 N)
â€“ It can also be converted to approximate nearest neighbour search to run faster, e.g.
â€¢ set an upper bound on the number of points to examine in the tree, or 
â€¢ interrupt the search based upon a real time clock 
(may be more appropriate in hardware implementations) 
â€¢ Some recommendations:
â€“ For small datasets and reduced dimensionality, brute force performs well
â€“ If data is sparse with reduced dimensionality (< 20), k-d tree performs better than ball-
trees
â€“ As the number of neighbours k increases, the query time of both k-d trees and ball-trees 
increases

Alberto Ortiz (last update 15/12/2025) 33
Contents
â€¢ k-Nearest Neighbours classifier
â€¢ Supplementary material: Nearest-Neighbour Search & k-d trees
â€¢ Condensed Nearest Neighbours
â€¢ Example of use

Alberto Ortiz (last update 15/12/2025) 34
Condensed Nearest Neighbour
â€¢ Another alternative to decrease the cost of an NN search is to reduce the size of the 
training set without significantly altering the classification accuracy. 
â€¢ Condensed Nearest Neighbour (CNN): 
Use a set of prototypes U ïƒ X to classify, instead of the full set X
1. Remove outliers: go through X, checking whether it would be recognized as the correct 
class; if not, then it is an outlier and it is removed from X
2. U = {random point from X}
3. Build the prototype set U: go through X, picking any point and checking whether it is 
recognized as the correct class according to U and 1-NN; if it is, then it is an absorbed 
point (i.e. interior point); if not, it is transferred to the prototype set U (i.e. border point)
4. Repeat 3 until no more prototypes are transferred to U
z = outlier
x, y = prototypes
q = absorbed
y
x
q
z
stage 1:
discard
outliers
stage 2:
choose
prototypes

Alberto Ortiz (last update 15/12/2025) 35
Condensed Nearest Neighbour
â€¢ An example (cntd.)
original dataset
- prototype
- outlier
- absorbed


Alberto Ortiz (last update 15/12/2025) 36
Contents
â€¢ k-Nearest Neighbours classifier
â€¢ Supplementary material: Nearest-Neighbour Search & k-d trees
â€¢ Condensed Nearest Neighbours
â€¢ Example of use

Alberto Ortiz (last update 15/12/2025) 37
Example of use
â€¢ Example with scikit-learn:
model = KNeighborsClassifier(n_neighbors, weights, algorithm, 
    leaf_size, metric, p)
n_neighbours: number of neighbours to be used for prediction, e.g. 5
weights: weight function used in prediction, posible values
â€¢ â€˜uniformâ€™: all points of the k-neighbourhood weigh the same
â€¢ â€˜distanceâ€™: points are weighted by the inverse of their distance
algorithm: â€˜autoâ€™, â€˜ball_treeâ€™, â€˜kd_treeâ€™, â€˜bruteâ€™
leaf_size: size of the leaf nodes for ball trees and kd-trees, e.g. 30
metric: â€˜euclideanâ€™, â€˜manhattanâ€™, â€˜chevyshevâ€™, â€˜minkowskiâ€™
p: power for the Minkowski metric, e.g. 2
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=5, weights=â€˜uniformâ€™, metric = 
â€˜euclideanâ€™)
clf.fit(X_train,y_train)
y_test = clf.predict(X_test)
https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

Instance-based learning:
k-Nearest Neighbours
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes