Unsupervised Learning:
Optimization-based clustering
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes

Alberto Ortiz (last update 19/01/2026) 2
Contents
â€¢ Introduction
â€¢ Membership-based clustering
â€¢ Possibilistic clustering
â€¢ Supplementary material: Mixture models

3
Introduction
â€¢ These clustering approaches (most popular in unsupervised learning) can be stated 
as the optimization of a cost function J using differential calculus techniques
â€¢ The cost function J is defined in terms of the data set X = {x1, x2, â€¦, xN} and the 
clustering R = {C1, C2, â€¦, CM}, for a predefined number of clusters M:
â€¢ Each cluster Cj is defined in terms of a set of parameters ï±j, which in turn depend on 
the features of the cluster we look for
â€“ e.g. ï±j can be a point in L-dimensional space corresponding to the centroid of the cluster
â€¢ Therefore, the problem becomes into the determination of the optimum set of 
parameters ï±:
â€¢ Due to this optimization-based nature, with these algorithms, all the samples of the 
data set ğ‘‹ are involved in the computation of the parameters of the clusters.
Alberto Ortiz (last update 19/01/2026)


Alberto Ortiz (last update 19/01/2026) 4
Contents
â€¢ Introduction
â€¢ Membership-based clustering
â€¢ Possibilistic clustering
â€¢ Supplementary material: Mixture models

5
Membership-based clustering
â€¢ In a first version, these algorithms define the cost function in the following terms:
â€“ where ğ‘¢ğ‘–ğ‘— = 1 if sample ğ‘¥ğ‘– is assigned to cluster ğ¶ğ‘—, otherwise ğ‘¢ğ‘–ğ‘— = 0
â€“ ğ½ is optimized with respect to ğ‘ˆ = {ğ‘¢ğ‘–ğ‘—} and ğœƒ = {ğœƒğ‘—}.
ïƒ The clustering problem becomes into the determination of optimum ğ‘ˆ and ğœƒ.
â€¢ if ïƒƒ is DM, the problem is min ğ½; if ïƒƒ is SM, the problem is max ğ½
â€¢ Example: given ğ‘ = 5 and ğ‘€ = 2, the following describes an optimum clustering  
Alberto Ortiz (last update 19/01/2026)
C1
C2
i.e. ğ‘¢11 = 1, ğ‘¢12 = 0

6
Membership-based clustering
â€¢ The previous problem, in which each sample belongs to exclusively one single 
cluster, is known as hard or crisp clustering:
â€¢ To find the optimum ğ‘ˆ = {ğ‘¢ğ‘–ğ‘—} and ğœƒ = {ğœƒğ‘—}, we can use differential calculus for ğœƒğ‘—, 
but not for ğ‘¢ğ‘–ğ‘—, because ğ½ is not differentiable with respect to ğ‘¢ğ‘–ğ‘— ïƒ {0,1}.
â€¢ Instead, we adopt the following (sub-optimum) rule:
â€“ Keeping ğœƒğ‘— constant, ğ‘— = 1, â€¦ , ğ‘€, since for each ğ‘¥ğ‘– only one ğ‘¢ğ‘–ğ‘— is 1 and the others are 0, 
we set
since ğ½(ğ‘‹; ğ‘ˆ, ğœƒ) is minimized if we assign each ğ‘¥ğ‘– to its closest/most similar cluster
Alberto Ortiz (last update 19/01/2026)


7
Membership-based clustering
â€¢ If we now keep constant ğ‘ˆ = {ğ‘¢ğ‘–ğ‘—}, we can find ğœƒ = {ğœƒğ‘—} using differential calculus 
and solving the resulting equation once ïƒƒ is chosen:
â€¢ We can now state the Generalized Clustering Hard Algorithmic Scheme (GCHAS):
Alberto Ortiz (last update 19/01/2026)
stage 1: estimate ğ‘¢ğ‘–ğ‘—
keeping ğœƒğ‘— at their 
previous values
stage 2: estimate ğœƒğ‘—
keeping ğ‘¢ğ‘–ğ‘— at their 
previous values
the process can be reversed:
set ğœƒ and next calculate ğ‘ˆ

8
Membership-based clustering
â€¢ The most popular instance of GCHAS is the so-called K-means algorithm 
(also known as C-means or Isodata)
â€¢ Each cluster is represented by its centroid, ğœƒğ‘— = ğœ‡ğ‘—, and ïƒƒ = ğ‘‘2(ğ‘¥ğ‘–, ğœ‡ğ‘—)2
Alberto Ortiz (last update 19/01/2026)


Alberto Ortiz / EPS (last update 19/01/2026) 9
Membership-based clustering
â€¢ Example: Two Gaussian classes such that
n1 = n2 = 50 âˆ’ ï­1 = (10,10), ï³1 = 2 âˆ’ ï­2 = (5,5), ï³2 = 1
initial centroids: blue squares
6 iterations
final centroids: ï­1 = (10.24, 9.89), ï­2 = (5.02, 4.90)

10
Membership-based clustering
â€¢ K-means is used widely and frequently finds reasonable solutions quickly
â€“ It is conceptually simple as well as its implementation
â€“ Its time complexity is O(ğ‘ğ‘€ğ‘), where ğ‘ is the number of iterations until convergence
â€¢ if ğ‘€ <<< ğ‘ and ğ‘ <<< ğ‘, K-means becomes eligible for processing large 
datasets
â€¢ However, there are also major shortcomings:
â€“ Although K-means has been proved not to increase ğ½ between iterations, convergence 
to the global optimum cannot be ensured (solve for the optimum is NP-hard)
â€¢ The outcome depends on the initial centroids
â€“ K-means is sensitive to outliers and noise
â€“ One or more clusters can be empty because of centroids too far away from any sample, 
i.e. the cluster has a representative but comprises no samples (due to bad initialization)
â€¢ To face these drawbacks:
â€“ One can run K-means several times (with different randomly chosen initial centroids) 
and keep the clustering leading to the lowest value of the cost function ğ½
â€“ K-means++ modifies the setup stage of K-means by a smarter initialization stage
â€“ K-medoids faces the outlier-sensitivity and naturally avoids empty clusters
Alberto Ortiz (last update 19/01/2026)

11
Membership-based clustering
â€¢ K-means++ as initialization of K-means
â€“ The intuition behind K-means++ is that spreading out the ğ‘€ initial cluster centroids is 
beneficial, e.g. at least decreases the number of iterations until convergence
â€“ The following pseudo-code assumes the use of a distance as the proximity measure:
Alberto Ortiz (last update 19/01/2026)


12
Membership-based clustering
â€¢ K-means++ initialization: Python code
Alberto Ortiz (last update 19/01/2026)
def initialize(X, K):
    S = [X[0]]
    for k in range(1, K):
        D2 = numpy.array([min([numpy.inner(s-x,s-x) for s in S]) for x in X])
        probs = D2/D2.sum() # x in S will lead to prob = 0 and will not be chosen
        cumprobs = probs.cumsum()
        r = scipy.rand()
        for j,p in enumerate(cumprobs):
            if r < p:
                i = j
                break
        S.append(X[i])
    return S example (of centroid selection):
probs = [0.1, 0.3, 0.2, 0.4]
cumprobs = [0.1, 0.4, 0.6, 1.0]
if r < cumprobs[0]:
# this event has probability 0.1
i = 0
elif r < cumprobs[1]: # r ï‚³ cumprobs[0]
# this event has probability 0.2
i = 1
elif r < cumprobs[2]: # r ï‚³ cumprobs[1]
# this event has probability 0.3
i = 2
elif r < cumprobs[3]: # r ï‚³ cumprobs[2]
# this event has probability 0.4
i = 3
0.0   0.1  0.2  0.3  0.4   0.5  0.6  0.7  0.8   0.9  1.0
r is generated uniformly in the inteval [0,1] ïƒ
10% probability of r falling in 1st interval
30% probability of r falling in 2nd interval
20% probability of r falling in 3rd interval
40% probability of r falling in 4th interval
40%20%30%10%

Alberto Ortiz / EPS (last update 19/01/2026) 13
Membership-based clustering
â€¢ Example: 4 Gaussian clases, 25 samples each


14
Membership-based clustering
â€¢ K-medoids â€“ PAM (Partioning Around Medoids) algorithm
â€“ Each cluster is represented by a sample from ğ‘‹, a medoid, instead of its centroid
â€“ Therefore, the cost function becomes:
where M is the set of samples which are medoids
â€“ At every iteration:
1. K-medoids assigns each sample to the closest medoid (ï‚º closest cluster)
2. K-medoids checks whether there exists a sample ğ‘¥ğ‘– which can replace a medoid ğ‘¥ğ‘—
and reduce ğ½
If this is possible, the medoid is redefined and a new iteration takes place; 
otherwise, the process stops
â€“ This procedure:
â€¢ avoids empty clusters, i.e. each cluster comprises at least one sample (= medoid),
â€¢ tends to be less sensitive to outliers and noise
â€“ CLARA and CLARANS are variants of PAM intended for dealing with large datasets
Alberto Ortiz (last update 19/01/2026)


15
Membership-based clustering
â€¢ A relaxation of constraint ğ‘¢ğ‘–ğ‘—ïƒ{0,1} leads to the so-called soft clustering problems
â€¢ One of these problems is the fuzzy clustering problem, whose formulation is:
where:
â€“ ïƒƒ is a DM for fuzzy clustering algorithms
â€“ ğ‘¢ğ‘–ğ‘— ïƒ [0,1] represents the grade of membership of sample ğ‘¥ğ‘– to cluster ğ¶ğ‘—
â€¢ ğ‘¢ğ‘–ğ‘— is related to ğ‘¢ğ‘–ğ‘˜, ğ‘˜ â‰  ğ‘— because of the constraints
â€“ ğ‘ is named as a fuzzyfier which gives more or less importance to ğ‘¢ğ‘–ğ‘—
ïƒ reduces the influence of ïƒƒ, what can be useful for dealing with outliers
Alberto Ortiz (last update 19/01/2026)
1
 
N
1                                M
i                                       =1
j
 
0 ï‚£ ï“ ï‚£ N
ï“
ï“
uij


16
Membership-based clustering
â€¢ Now, we can find the optimum ğ‘ˆ and ğœƒ using differential calculus:
â€“ To find the optimum ğ‘ˆ = {ğ‘¢ğ‘–ğ‘—} we need to introduce the following Lagrangian function 
because of the constraints on ğ‘¢ğ‘–ğ‘—:
â€“ The partial derivative of L(ğ‘‹; ğ‘ˆ, ğœƒ) with respect to ğ‘¢ğ‘Ÿğ‘  is
â€“ We can now equal to 0 and solve for ğ‘¢ğ‘Ÿğ‘ :
Alberto Ortiz (last update 19/01/2026)


17
Membership-based clustering
â€¢ Now, we can find optimum ğ‘ˆ and ğœƒ using differential calculus:
â€“ To find the optimum ğœƒ = {ğœƒğ‘—} we calculate the partial derivative with respect to ğœƒğ‘—and 
equal it to 0:
â€¢ Summing up, we have:
â€¢ There are two issues with the expressions we have just obtained: 
â€“ The expression leading to the calculation of ğœƒ = {ğœƒğ‘—} depends on the particular proximity
function we are using, so the derivative and the underlying equation cannot be solved in 
general
â€“ Besides, the two expressions are coupled, what prevents from obtaining closed-form 
expressions
â€¢ REMARK: Notice that
Alberto Ortiz (last update 19/01/2026)


18
Membership-based clustering
â€¢ One way of proceeding is to employ a two-stage iterative algorithm named GCFAS
â€“ the algorithm can also be started from ğ‘ˆ(0) instead of ğœƒ(0)
â€“ ğ‘¢ğ‘–ğ‘— cannot be calculated for ğ‘¥ğ‘– if ï€¤ğ‘— such that ïƒƒ(ğ‘¥ğ‘– , ğœƒğ‘—(ğ‘¡)) = 0, e.g. ïƒƒ is DM
â€¢ check this case and set ğ‘¢ğ‘–ğ‘— so that ğ‘¥ğ‘– is arbitrarily shared among clusters ğ¶ğ‘—
Alberto Ortiz (last update 19/01/2026)
stage 1: estimate ğ‘¢ğ‘–ğ‘—
keeping ğœƒğ‘— at their 
previous values
stage 2: estimate ğœƒğ‘—
keeping ğ‘¢ğ‘–ğ‘— at their 
previous values

19
Membership-based clustering
â€¢ If ğœƒğ‘— is a point representative of cluster ğ¶ğ‘—, e.g. its (fuzzy) centroid, and 
ïƒƒ(ğ‘¥ğ‘–, ğœƒğ‘—(ğ‘¡)) is the squared Euclidean distance then:
â€“ The resulting algorithm is named Fuzzy c-Means (FCM) or Soft / Fuzzy k-Means
â€“ This result is also valid if
for ğ´ symmetric, positive definite (i.e. all eigenvalues are positive)
e.g. ğ´âˆ’1 can be the fuzzy covariance matrix of ğ¶ğ‘— : 
In this case, ïƒƒ(ğ‘¥ğ‘–, ğœƒğ‘—(ğ‘¡)) is termed as the Mahalanobis distance
Alberto Ortiz (last update 19/01/2026)


20
Membership-based clustering
â€¢ Fuzzy c-Means clustering algorithm (FCM):
Alberto Ortiz (last update 19/01/2026)


21
Membership-based clustering
â€¢ Example: N = 140 samples, M = 2 clusters, q = 2
Alberto Ortiz (last update 19/01/2026)


22
Membership-based clustering
â€¢ Other FC algorithms: Gustafson-Kessel algorithm for hyperplanes clustering
â€“ Planar clusters are represented by centers ğ‘ğ‘— and covariance matrices ï“ğ‘—, i.e. ğœƒğ‘— = (ğ‘ğ‘— , ï“ğ‘—)
â€“ Following the same derivation procedure as before:
Alberto Ortiz


23
Membership-based clustering
â€¢ Other FC algorithms: hyperellipsoids clustering
â€“ Adaptive Fuzzy C-Shells clustering (AFCS), where a hyperellipsoid is represented by its 
center ğ‘ğ‘— and its shape, defined by a symmetric, positive definite matrix ğ´ğ‘—, i.e. ğœƒğ‘— = (ğ‘ğ‘—, ğ´ğ‘—)
â€“ ğ‘‘ğ‘›ğ‘Ÿ is the normalized radial distance 
(distance to the center of the hyperellipsoid)
â€“ very similar to hyperplanes clustering: ğ´ğ‘— = ï“ğ‘—
âˆ’1
â€“ another alternative: Fuzzy C Ellipsoidal shells (FCES)
Alberto Ortiz (last update 19/01/2026)
initial guess
final estimate

24
Membership-based clustering
â€¢ Last remarks about membership-based clustering:
â€“ K-means is considered as hard clustering (HC) against FCM which is considered as soft 
clustering (SC) as a result of the relaxation of the membership-related constraints
â€¢ Actually, K-means can be regarded as a particular case of FCM
â€“ HC algorithms are not as robust as their SC counterparts when other than point 
representatives are used, e.g. the hard-clustering version of the GK algorithm for 
hyperplanes clustering needs an adequate number of samples from all underlying 
clusters to avoid degenerate cases where Î£ğ‘— is not invertible
â€¢ If the descriptor of a sample ğ‘¥ğ‘– changes slightly, an HC algorithm might lead to 
reassigning ğ‘¥ğ‘– from one cluster to another, 
while, for the case of a SC algorithm, the membership values ğ‘¢ğ‘–âˆ— might be modified 
but drastic changes will be more difficult to occur
â€“ In general terms, the fuzzy concepts embedded in SC algorithms make them more 
flexible than HC algorithms, i.e. able to deal better with noise and data uncertainty
â€¢ By means of FCM, we obtain soft assignments from samples to clusters, what in turn 
reflects assignments uncertainty (because of the data uncertainty itself) 
over the most appropriate assignment
Alberto Ortiz (last update 19/01/2026)

Alberto Ortiz (last update 19/01/2026) 25
Contents
â€¢ Introduction
â€¢ Membership-based clustering
â€¢ Possibilistic clustering
â€¢ Supplementary material: Mixture models

26
Possibilistic clustering
â€¢ The formulation for the possibilistic clustering (PC) problem is similar to FC:
where:
â€“  ïƒƒ is a DM for possibilistic clustering algorithms
â€“ ğ‘¢ğ‘–ğ‘— now represents the grade of compatibility of sample ğ‘¥ğ‘– with cluster ğ¶ğ‘— or 
the possibility that sample ğ‘¥ğ‘– belongs to cluster ğ¶ğ‘—
â€¢ ğ‘¢ğ‘–ğ‘— is no longer coupled to ğ‘¢ğ‘–ğ‘˜, ğ‘˜ â‰  ğ‘—, because the constraints have changed
â€¢ the possibility that ğ‘¥ğ‘– belongs to ğ¶ğ‘— depends on exclusively ğ‘¥ğ‘– and ğœƒğ‘—
â€¢ it is thus independent of the possibilities that ğ‘¥ğ‘– belongs to any other cluster
â€“ q does not play the same role as in FC, we will clarify this later
â€“ PC behaves better than FC for noisy datasets and 
depends less on a good election of M
Alberto Ortiz (last update 19/01/2026)
1
 
N
1                                M
i                                       > 0
j
 
0 < ï“ ï‚£ N
max
ï“
uij
ğ‘ . ğ‘¡. à·
ğ‘—=1
ğ‘€
ğ‘¢ğ‘–ğ‘— â‰¤ 1 â†’
makes ğ‘¢ğ‘–ğ‘— depend 
on each other

27
Possibilistic clustering
â€¢ The direct optimization of the previous cost function leads to the trivial zero-solution
â€¢ In order to avoid that situation, we have to incorporate an additional term into J:
â€“ The additional term depends on ğ‘¢ğ‘–ğ‘— to avoid the zero-solution and also to reduce the 
effect of outliers
â€“ ï¨ğ‘— are suitably chosen positive constants, which will be discussed later
â€¢ To find the optimum ğ‘ˆ = {ğ‘¢ğ‘–ğ‘—} we calculate the partial derivative with respect to ğ‘¢ğ‘–ğ‘—
and equal it to 0:
â€“ ğ‘¢ğ‘–ğ‘— is inversely dependent onïƒƒ(ğ‘¥ğ‘–, ğœƒğ‘—), what in effect reduces the influence of outliers
â€¢ Since the second term of ğ½ does not involve the cluster representatives ğœƒğ‘—, one may 
conclude that the updating of ğœƒğ‘— is carried out the same way as for FC.
Alberto Ortiz (last update 19/01/2026)


28
Possibilistic clustering
â€¢ Generalized Clustering Possibilistic Algorithmic Scheme (GCPAS):
â€“ the algorithm can also be started from ğ‘ˆ(0) instead of ğœƒ(0)
â€“ there are no longer issues with ïƒƒ(ğ‘¥ğ‘–, ğ‘ğ‘—(ğ‘¡)) = 0
â€“ all FC algorithms can be transformed into their PC counterparts: PCM, PGK, APCS, 
PCES
Alberto Ortiz (last update 19/01/2026)


29
Possibilistic clustering
â€¢ Possibilistic c-Means clustering algorithm (PCM):
Alberto Ortiz (last update 19/01/2026)


30
Possibilistic clustering
â€¢ Role of q:
â€“ q determines the kind of â€œcompatibilityâ€ 
function implemented by the algorithm
â€“ ğ‘ = 1 â‡’ ğ‘¢ğ‘–ğ‘— = 0, ï€¢ğ‘¥ğ‘– | ïƒƒ(ğ‘¥ğ‘– , ğœƒğ‘—) > ï¨ğ‘—
â€“ ğ‘ â†’ +âˆ â‡’ ğ‘¢ğ‘–ğ‘— tends to constant 0.5, ï€¢ğ‘¥ğ‘–
â€¢ Role of ï¨ğ’‹:
â€“ ï¨ğ’‹ determines the dissimilarity level between ğ‘¥ğ‘– and ğ¶ğ‘— at which ğ‘¢ğ‘–ğ‘— = 0.5
ïƒ defines the â€œsizeâ€ and â€œshapeâ€ of the cluster
ïƒ defines the influence of a specific point ğ‘¥ğ‘– on the estimation of the ğ¶ğ‘— representative
â€“ one way to estimate the value of ï¨ğ’‹ is to run FCM and after its convergence
Alberto Ortiz (last update 19/01/2026)


31
Possibilistic clustering
â€¢ PCM as mode-seeking algorithm:
ïƒ minimization of ğ½ requires maximizing ğ‘¢ğ‘–ğ‘—, which, in turn, requires minimization of ïƒƒ(ğ‘¥ğ‘–, ğ‘ğ‘—)
ïƒ if we run PCM for ğ‘€ clusters but ğ‘‹ comprises ğ¾ < ğ‘€ natural clusters, some of the ğ‘€
clusters will coincide with others, and hence the number of clusters in ğ‘‹ need not be known 
a priori 
ïƒ ğ¶ğ‘— are finally placed in regions dense in samples, i.e. the modes of the dataset
Alberto Ortiz (last update 19/01/2026)


32
Possibilistic clustering
â€¢ Alternative GCPAS:
â€“ An alternative possibilistic scheme can be derived from a new cost function:
â€¢  ğ‘ is no longer involved
â€“ It can be shown that, for this case:
â€¢  ğ‘¢ğ‘–ğ‘— decreases more rapidly than in the standard scheme
Alberto Ortiz (last update 19/01/2026)


33
Possibilistic clustering
â€¢ Example (standard PCM): N = 140 samples, M = 2 clusters, q = 2, ï¨j set by FCM
Alberto Ortiz (last update 19/01/2026)


Alberto Ortiz (last update 19/01/2026) 34
Contents
â€¢ Introduction
â€¢ Membership-based clustering
â€¢ Possibilistic clustering
â€¢ Supplementary material: Mixture models

35
Mixture models
â€¢ These algorithms assume that there are M clusters underlying the dataset, each 
coming from a population obeying a certain probability distribution, so that the goal is 
to discover the parameters of the mixture. They are also termed as soft clustering.
â€¢ We intend to discover the parameters of the three Gaussians (ï­i, ï“i) and the 
assignments of samples to clusters.
Alberto Ortiz (last update 19/01/2026)
Example: We have a dataset 
such as:
â€¦ and, in effect, there are 3 clusters, everyone
coming from a different Gaussian:
We guess there are M = 3
clusters normally distributed â€¦

36
Mixture models
â€¢ We adopt an optimization-based approach which assigns each sample xi to the cluster with 
highest likelihood: 
and we model the probability of each sample introducing weights ï°j:
since, a priori, we do not know which sample belongs to which cluster,
where:
so that:
â€¢ Therefore, the output of the optimization process is the parameters ï±j
of the clusters and the weights rij
â€“ e.g. for the previous example, we can plot samples xi using a 
colour code proportional to the corresponding rij â†’
Alberto Ortiz (last update 19/01/2026)


37
Mixture models
â€¢ Adopting a log-likelihood approach and assuming i.i.d samples, J is defined as:
â€¢ The maximum likelihood 
optimization problem hence 
becomes:
â€“ The elements of the N ï‚´ M matrix Z = {zij} are known as latent variables in this problem.
â€“ rij is named as the responsibility that cluster Cj takes for explaining xi.
â€“ ï°j are the mixing coefficients, which somehow can be seen as the amount of 
explanation of the data performed by cluster Cj.
Alberto Ortiz (last update 19/01/2026)


38
Mixture models
â€¢ The mixture problem can be solved by means of the Expectation-Maximization
(EM) algorithm.
â€“ EM is an iterative algorithm that comprises two steps:
â€¢ The E step, where the latent variables zij are re-estimated (keeping constant the 
parameters of the clusters)
â€¢ The M step, where the distribution parameters are re-estimated (keeping constant 
the latent variables zij)
â€“ EM is useful to solve other problems apart from clustering on the basis of a mixture model
â€¢ The most widely used mixture model is the mixture of Gaussians (MOG), also 
called a Gaussian mixture model (GMM):
â€¢ Because of the shape of a Gaussian pdf, this algorithm is useful for detecting 
compact and hyperellipsoidal clusters
Alberto Ortiz (last update 19/01/2026)


39
Mixture models
â€¢ Another example of Mixture of Gaussians:
Alberto Ortiz (last update 19/01/2026)


40
Mixture models
â€¢ Algorithm EM for Gaussian mixtures:
Alberto Ortiz (last update 19/01/2026)
E-step: estimate rij
keeping ï±j and ï°j at 
their previous 
values
M-step: estimate ï±j
and ï°j keeping 
rij at their previous 
values


41
Mixture models
â€¢ Example: rescaled Old Faithful data set, M = 2
Alberto Ortiz (last update 19/01/2026)
t = 1
t = 2 t = 5 t = 20
before E(1) after E(1) after M(1)

42
Mixture models
â€¢ Final remarks:
â€“ Remember that, once the mixture model has been fitted, we can make use of
for â€œhardâ€ sample allocation to clusters
â€“ The E-step can be computationally demanding because of the calculations of ï“j
-1.
â€¢ One way to relax this is to assume that all covariance matrices are diagonal or that 
they are all equal to each other (only one inversion is required at each iteration step).
â€“ The EM-GMM algorithm reduces to K-means when ï“j = ï³^2I and ï°j = 1/M.
Alberto Ortiz (last update 19/01/2026)


Unsupervised Learning:
Optimization-based clustering
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes