Lecture 6:
Assessment of machine
(supervised) learning systems
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes


Alberto Ortiz (last update 10/11/2025) 2
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints

3
Introduction
â€¢ The assessment of ML systems involves 
several aspects of its performance and
may take the designer back to any of the
developing stages
â€¢ Regarding the model itself, we may evaluate
â€“ its adequacy
â€¢ is the model adequate? is it biased? 
which is the error rate?
â€“ its generalization
â€¢ how well behaves the model with unseen data
â€¢ ML systems evaluation tools are useful not only for assessing the performance of the 
system, but also 
â€“ for debugging purposes, to figure out what is not working and make the necessary 
adjustments
â€“ to compare among different ML models
â€¢ The basic assessment protocol splits the dataset into: 
â€“ Training set. Used to build the classifier/regressor.
â€“ Test set. To check the behaviour of the classifier with unseen examples.
collection of data
initial analysis
data preparation
design of ML system
tuning of ML system
performance assessment
underfitting 
vs 
overfitting
Alberto Ortiz (last update 10/11/2025)

4
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints
Alberto Ortiz (last update 10/11/2025)

5
Bias-variance tradeoff
â€¢ The source of prediction errors can be shown to be three-fold:
â€“ Bias
â€¢ It is due to wrong assumptions, either by the designer or by the model
â€¢ A high-bias model is most likely to underfit the training data
â€“ Variance
â€¢ It is due to the modelâ€™s excessive sensitivity 
to small variations in the training data
â€¢ A model with many degrees of freedom, 
e.g. a high-degree polynomial, is likely to have 
high variance and thus to overfit the training data
â€“ Irreducible error
â€¢ It is due to the noisiness of the data itself
â€¢ the only way to reduce this error is to clean up 
the data, i.e. fix a broken sensor
â€¢ Increasing a modelâ€™s complexity typically 
increases its variance and reduces its bias
â€¢ Reducing a modelâ€™s complexity usually 
increases its bias and reduces its variance
â€¢ This is why it is called the 
bias-variance tradeoff (also BV dilemma)
test error
training 
error
model complexity
Alberto Ortiz (last update 10/11/2025)

6
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints
Alberto Ortiz (last update 10/11/2025)

7
Confusion matrix
â€¢ Many performance metrics can be calculated from the confusion matrix, 
e.g. for two classes, having defined first which is the positive class
Alberto Ortiz (last update 10/11/2025)


8
Confusion matrix
â€¢ F1 -score combines in a single metric P and R, by means of their harmonic mean
2
Î¤1 ğ‘ƒ + Î¤1 ğ‘… = 2ğ‘ƒğ‘…
ğ‘ƒ + ğ‘…
â€“ the regular mean treats all values equally, the harmonic mean penalizes more low values
â€“ a high F1 score (which is better) results only if both P and R are high
â€“ also known as Sorensen-Dice coefficient or Dice Similarity Coefficient (DSC)
â€¢ F1 is a particular case of the F - score or Fï¢ - score:
â€“ ï¢ = 1,  F1 -score, which weighs equally R and P: same emphasis on FN and FP
â€“ ï¢ = 2,  F2 -score, which weighs R lower than P: effect of FP is less noticeable
â€“ ï¢ = 0.5, F0.5 -score, which weighs R higher than P: effect of FN is less noticeable
Alberto Ortiz (last update 10/11/2025)

9
Confusion matrix
â€¢ The confusion matrix can be generalized for M-class problems
â€¢ Global metrics can be calculated following two approaches:
â€“ macro-averages: average scores for each class
â€¢ each class is weighed equally
â€“ micro-averages: from individual TP, TN, FP, FN
â€¢ each prediction is weighed equally
TN TNFP
TN
FN
FP TN
FNTP
C0â€¦Ck-1  Ck  Ck+1â€¦Cm
Cm â€¦ Ck+1  Ck  Ck-1 â€¦ C0
predicted class
true class
TP, TN, FP, FN are calculated according 
to one versus all (OvA) classification
Alberto Ortiz (last update 10/11/2025)
Pğ‘˜ =
TPğ‘˜
TPğ‘˜+FPğ‘˜
, etc.
e.g. Pmicro =
TP1+â‹¯+TPğ‘š
TP1+â‹¯+TPğ‘š+FP1+â‹¯+FPğ‘š
e.g. Pmacro =
P1+â‹¯+Pğ‘š
ğ‘š

10
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints
Alberto Ortiz (last update 10/11/2025)

11
Cross-validation techniques
â€¢ From a methodological point of view, we need a way to obtain robust and bias-free 
performance measurements
â€¢ Cross-validation techniques can provide performance estimation values with low 
bias:
â€“ holdout cross validation
â€“ n-fold cross validation
â€“ stratified n-fold cross validation
â€“ leave-one-out cross validation (LOOCV)
â€“ nested cross validation
Alberto Ortiz (last update 10/11/2025)

12
Cross-validation techniques
â€¢ Holdout cross validation
â€“ Simplest kind of cross validation
â€“ The data set is initially split into two sets: 
the training set and the test set 
1. ML system is built using the training set only
2. ML system is asked to predict the output values 
for the data in the â€œunseenâ€ test set
â€“ The accumulated errors give the test set error, 
used to evaluate the model
â€¢ Maybe high variance in the results if the test is repeated
â€“ To tune the system appropriately and reduce this variance, 
we can split the training set in 
a training subset and a validation subset
â€¢ The validation subset can be employed for model selection, i.e. tune hyperparameters
â€“ train the system
â€“ repeatedly evaluate it using the validation subset using different settings
â€“ Even in this way, the evaluation may depend heavily on which data points end up in the 
training set and which end up in the test set, and thus the evaluation may be 
significantly different depending on how the splitting is made
original dataset
training test
training testvalidation
ML system
prediction
tuning system 
perform
ance
Alberto Ortiz (last update 10/11/2025)

13
Cross-validation techniques
â€¢ n-fold cross validation
â€“ The available data is randomly split into n folds
without replacement:
â€¢ ğ‘› âˆ’ 1 folds are used for training
â€¢ the remaining fold is used for testing
â€“ The procedure is repeated ğ‘› times, choosing a
different fold for testing each iteration (no overlapping)
â€“ A global performance measure is obtained by
averaging the individual measurements
â€¢ lower variance estimate than the holdout method
â€“ In most cases, ğ‘› = 5 or 10
â€¢ ğ‘› large ïƒ more data for training ïƒ lower bias in E but longer runtime
â€“ A high variance indicates a situation of overfitting
â€“ Stratified n-fold cross validation
â€¢ Class proportions are preserved in each fold
â€¢ Better performance estimates as for bias and variance
â€“ Leave-one-out cross validation (LOOCV)
â€¢ ğ‘› = ğ‘, the size of the dataset; hence, at each iteration, the test set comprises one single sample
â€¢ recommended for small datasets
Alberto Ortiz (last update 10/11/2025)
available data
1)
2)
3)
10)
E1
E2
E3
E10
E =
1
ğ‘› Ïƒi=1
n  Ei  
V = Var( Ei ) 

14
Cross-validation techniques
â€¢ Nested cross validation
â€“ Outer n-fold: performance estimation
the available data is randomly split into 
n folds without replacement:
â€“ ğ‘› âˆ’ 1 folds are used for training
â€“ the remaining fold is used for test
â€¢ The procedure is repeated ğ‘› times, 
choosing a different fold for testing 
each iteration
â€“ Inner m-fold: model selection (hyperp. tuning)
the set of training folds is split into ğ’ folds
leaving one for validation 
â€“ A global performance measure is obtained by
averaging the individual measurements
â€“ This measure gives a good estimate of what 
to expect from unseen data
available data
1)
2)
3)
10)
E1
E2
E3
E10
E=
1
ğ‘› Ïƒi=1
n Ei  10 ï‚´ 3 cross-validation
inner loop: 
parameter tuning outer loop: 
train with optimal params.
Alberto Ortiz (last update 10/11/2025)

15
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints
Alberto Ortiz (last update 10/11/2025)

16
ROC curves
â€¢ Receiver Operating Characteristic curve (concept from early Radar days)
â€“ Set of (FPR, TPR) points obtained by varying the algorithmâ€™s parameters
â€¢ Every (FPR, TPR) point is 1 classifier configuration, choose the closest to the (0,1) point
â€¢ Can also be shown as sensitivity (TPR) vs 1 â€“ specificity (FPR)
â€“ Area Under the Curve (AUC)
â€¢ Global measure of classifier performance, the higher the better
(FPR=0, TPR=1)
perfect classifier random guessing: 
FPR = TPR ïƒ 
FP / Ne = TP / Po ïƒ
FP / TP = Ne / Po
Ne / Po = 1 ïƒ TP = FP
Area Under the Curve (AUC)
Alberto Ortiz (last update 10/11/2025)

17
ROC curves
â€¢ Receiver Operating Characteristic curve (concept from early Radar days)
â€“ can also be calculated for multi-class problems
Alberto Ortiz (last update 10/11/2025)

18
ROC curves
â€¢ Other usual curves for classifier performance characterization are the 
Precision-Recall curves
â€“ The AUC is also of application in this case, also maximum R for P = 1, etc.
â€“ Note: TN are not accounted for in this curve, it is the kind of problem where negatives are 
not as relevant as positives, e.g. inspection systems
(ğ‘… = 1, ğ‘ƒ = 1)
perfect classifier
Alberto Ortiz (last update 10/11/2025)

Alberto Ortiz (last update 10/11/2025) 19
Contents
â€¢ Introduction
â€¢ Bias-variance tradeoff
â€¢ Confusion matrix and performance metrics
â€¢ Assessment methodologies: Cross-validation techniques
â€¢ ROC curves
â€¢ Debugging and model tuning hints

20
Bias and variance problems
â€¢ By plotting the model training and 
validation accuracies as functions 
of the training set size (learning curves), 
we can easily detect whether the model 
suffers from high variance or high bias
â€“ high bias
â€¢ symptom: 
low training and validation accuracy
and hence the model underfits the data
â€¢ solution:
increase model complexity either 
with more parameters or 
more features
â€“ high variance
â€¢ symptom: large gap between training and validation accuracy, the model captures 
too well the training data (overfitting)
â€¢ solution: collect more training data (care with noisy data, performance will not 
improve), reduce the model complexity (e.g. regularization) or remove some features 
(via feature selection or extraction)
progressively increase training set size, train and test
Alberto Ortiz (last update 10/11/2025)

21
Model tuning
â€¢ In machine learning, we have two types of parameters:
â€“ those that are learned from training data, e.g. separating hyperplane
â€“ those that are optimized separately, e.g. decision tree depth or C in soft SVM
â€¢ The latter are known as hyperparameters
â€“ They can be set one by one, 
using performance (train and 
validation) learning curves
â€“ or via grid search
â€¢ brute-force exhaustive search paradigm where we specify a list of values for 
different hyperparameters and the computer evaluates the model performance for 
each combination, e.g. when optimizing SVM 
â€“ kernel type: linear, (in)homogeneous polynomial, rbf
â€“ kernel parameters: polynomial degree q, coefficient ï§
â€“ soft SVM hyperparameter C
Alberto Ortiz (last update 10/11/2025)

Lecture 6:
Assessment of machine
(supervised) learning systems
Alberto ORTIZ RODRÃGUEZ
11752 Aprendizaje AutomÃ¡tico
11752 Machine Learning
MÃ¡ster Universitario
en Sistemas Inteligentes
